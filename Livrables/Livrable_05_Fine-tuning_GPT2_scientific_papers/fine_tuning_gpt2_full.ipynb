{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning du modèle GPT-2 sur des articles scientifiques\n",
        "\n",
        "Dans ce notebook, nous nous sommes focalisés sur l'entrainement par fine-tuning du modèle GPT-2, sur un jeu de données d'articles scientifiques scientific_papers (dataset Tensorflow).\n",
        "\n",
        "L'objectif est de pouvoir spécialiser un modèle générique (modèle GPT-2 base) sur un domaine spécifique via un entrainement sur un jeu de données fourni (dataset TensorFlow scientific_papers)\n",
        "\n",
        "**Groupe ALOQAS**\n",
        "- Aurélien ZUFIC\n",
        "- Lucas AGUETAÏ\n",
        "- Ony ANDRIATSAHAVOJAONA\n",
        "- Quentin VERMEERSCH\n",
        "- Alexandre HUYNH\n",
        "- Samuel DORISMOND\n",
        "\n",
        "**NOTE /!\\ : L'entrainement s'est effectué en deux temps (TRAIN PART 1 & 2), étant donné qu'il était impossible de charger et entraîner le modèle sur le jeu de données en entier (limitations mémoire RAM). Il sera nécessaire de commenter et décommenter certaines sections de code pour l'exécution en cas d'entrainement à partir du début.**"
      ],
      "metadata": {
        "id": "iDnh-7bqe5OK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Import de packages\n",
        "Importation des packages Python nécessaires pour le fine-tuning.\n",
        "\n",
        "Nous chargeons également un package personnel datasets_scientific_paper, contenant des fonctions Python écrites pour le chargement de la dataset localement.\n",
        "\n",
        "Celle-ci est divisée en plusieurs parties à travers plusieurs fichiers, afin de prendre en compte les performances limités de Google Colaboratory."
      ],
      "metadata": {
        "id": "dcYyAamZjWau"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7VlbHoVEVuW"
      },
      "outputs": [],
      "source": [
        "!pip install keras_nlp -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAE0hOgcEWgy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c1efdd-4e7e-4bae-fafd-aa924adbd721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "import os\n",
        "import keras_nlp\n",
        "import time\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "sys.path.append('/content/drive/MyDrive/package')\n",
        "import datasets_scientific_paper as load_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAvIWpeOWp-2",
        "outputId": "0bf946ce-3b86-4d01-c822-5383c9bdec93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Chargement dataset train en local\n",
        "\n",
        "Pour le fine-tuning du modèle GPT-2, nous utilisons pour l'instant seulement les articles d'entrainement (le jeu de données entier d'origine est divisée en 3 parties : train, val, test).\n",
        "\n",
        "Le package importée précédemment nous permet d'utiliser une fonction personnalisée load_dataset pour charger dans des variables Python le jeu de données sous forme de dictionnaires et de listes."
      ],
      "metadata": {
        "id": "Om39z_F-jaa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# chemin vers le jeu de données divisée localement\n",
        "pathToDataset = \"drive/MyDrive/chunking-dataset\""
      ],
      "metadata": {
        "id": "Ldl9hh1TBRIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOoV6iUVep-Y"
      },
      "outputs": [],
      "source": [
        "# \"train\" afin de prendre seulement la partie du jeu de données dédié à l'entrainement du modèle\n",
        "# 100 afin de prendre 100 fichiers de 10.000 articles\n",
        "\n",
        "# === TRAIN PART 1 ===\n",
        "#train_data, train_labels = load_ds.load_dataset(pathToDataset, \"train\", 81)\n",
        "\n",
        "# === TRAIN PART 2 ===\n",
        "train_data, train_labels = load_ds.load_dataset(pathToDataset, \"train_extract_3\", 41)\n",
        "# en raison des limitations mémoires sur Colab\n",
        "# utilisation d'un autre répertoire créé manuellement : chunking-dataset/train_extract_3\n",
        "# copier manuellement les fichiers train_part-80 à train_part-119 dans ce répertoire\n",
        "# afin de charger seulement les articles des fichiers 80 à 119"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Initialisation d'un modèle GPT-2 de base\n",
        "Nous initialisons une instance du modèle GPT-2 sur lequel nous effectuons l'entrainement.\n",
        "\n",
        "Le modèle et code utilisés sont celui proposés via Keras NLP.\n",
        "\n",
        "Il existe plusieurs modèles préentrainées pour GPT-2, ayant un nombre de couches et de paramètres différent : celui sélectionné dans notre cas est le modèle gpt2_base_en, qui est suffisant pour notre entrainement."
      ],
      "metadata": {
        "id": "d_kKCDSYjfU1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dooIDfMeMSL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4eafb08-b28b-43d2-f24f-0a108798eca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py:37: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py:37: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # or \"tensorflow\" or \"torch\"\n",
        "\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    sequence_length=128,\n",
        ")\n",
        "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "    \"gpt2_base_en\", preprocessor=preprocessor\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous définissons une fonction articles_to_text nous permettant de concaténer tous les mots des articles dans une liste Python unique.\n",
        "\n",
        "En effet, chaque article possèdent plusieurs champs d'attributs : article_text, abstract, section_names...\n",
        "\n",
        "Le plus important pour le fine-tuning est d'entrainer le modèle sur le contenu textuel, donc article_text. Cette dernière - après chargement via notre fonction personnalisée - est composée pour chaque article d'une liste Python constitué de sous listes python pour plusieurs portions du texte. La fonction présente reformatte cela afin de ne garder qu'une seule liste Python contenant tous les mots sur lequel l'apprentissage doit se faire."
      ],
      "metadata": {
        "id": "xpcYMaN9jQFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def articles_to_text(articles, text_key, n_start, n_end):\n",
        "  article_list = []\n",
        "  for article in articles[n_start:n_end] :\n",
        "    article_text = \" \".join(article.get(text_key, \"\"))\n",
        "    article_list.append(article_text)\n",
        "  return article_list"
      ],
      "metadata": {
        "id": "uXYqaQ_RiWeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Chargement des paramètres d'entrainement\n",
        "\n",
        "Il existe plusieurs méthodes pour l'enregistrement des résultats de l'entrainement d'un modèle sur un support fichier.\n",
        "\n",
        "Basé sur les enseignements fournis par notre enseignant M. Faye, nous avons retenus trois méthodes :\n",
        "- **Fichier .keras** : enregistrement du **modèle** (architecture, couches)\n",
        "- **Fichier .ckpt** : enregistrement des **paramètres / poids** mobilisés\n",
        "- **Fichier .h5** : enregistrement du **modèle** et des **poids** mobilisés\n",
        "\n",
        "Il nous a été recommandé pour le fine-tuning de préférer l'enregistrement des paramètres. Il nous suffit donc à chaque début d'initialiser le modèle GPT-2 de base et de charger l'entrainement effectué via le chargement des poids/paramètres (fonction .load_weights)."
      ],
      "metadata": {
        "id": "t_O8TCfIjv3U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6ssSaskqVjw"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/training_data_all_3/cp.ckpt\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement des poids du modèle (facultatif)\n",
        "# - exécuter seulement si des fichiers de checkpoints existent\n",
        "gpt2_lm.load_weights(checkpoint_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88n5_M164HVU",
        "outputId": "021cf556-3140-454d-f0c7-afb990f33e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/task.py:47: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/task.py:47: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7a634c1289d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Entrainement / Fine-tuning\n",
        "\n",
        "C'est dans cette partie que le fine-tuning du modèle GPT-2 est réalisé.\n",
        "\n",
        "## Principe\n",
        "\n",
        "Le principe de ce fine-tuning est de fournir un grand nombre de mots et de phrases d'articles scientifiques dans un tableau Python qui sera fourni à une fonction .fit pour l'entrainement du modèle. Le modèle va se charger d'apprendre sur ce qui est fourni afin d'orienter la génération de texte du modèle GPT-2 vers un vocabulaire et style similaire aux articles scientifiques.\n",
        "\n",
        "Dans ce code, si des fichiers de checkpoint sont trouvés, le programme charge les poids existants, sinon il entraine le modèle et sauvegarde de nouveaux poids.\n",
        "\n",
        "## Entrainement sur échantillons d'articles scientifiques\n",
        "\n",
        "Nous fournissons à la fonction .fit réalisant l'entrainement une liste Python features, auquel on donne comme liste le contenu textuel d'articles obtenus via la fonction articles_to_text, mentionnée précédemment.\n",
        "\n",
        "Plusieurs essais ont été menés progressivement avec des échantillons de nombre croissant. Pour s'assurer que l'entrainement était faisable compte tenu des performances et ressources à notre disposition, nous avions débuté avec 1 à 10 articles.\n",
        "\n",
        "Nous avons procédé par la suite à des nombres d'articles plus importants : 10.000 articles, 50.000, et enfin 80.000 articles, limite atteinte d'articles mobilisables dans un même entrainement.\n",
        "\n",
        "## Callback ModelCheckpoint\n",
        "\n",
        "Un callback est utilisé afin d'effectuer une sauvegarde des paramètres à chaque \"checkpoint\" (point de contrôle) en fin d'epoch. Etant donné que nous avons défini seulement 1 seul epoch, l'enregistrement des poids se fait en fin d'entrainement. <br />\n",
        "Cela entraîne la création de fichiers .ckpt constituant l'entrainement du modèle, sous la forme de divers poids pour le modèle GPT-2.\n",
        "\n",
        "## Mobilisation du GPU\n",
        "\n",
        "Afin de permettre la réalisation de l'entrainement en évitant la saturation de la mémoire vive, il était nécessaire de mobiliser le GPU en plus de la RAM.\n",
        "\n",
        "Nous avons donc inclus notre dans une structure Python spécifique qui nous permet de solliciter le GPU pour une partie spécifique du code.\n",
        "```python\n",
        "with tf.device('/device:GPU:0'):\n",
        "```\n"
      ],
      "metadata": {
        "id": "0tgjkTtuj_av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Définir le chemin et le répertoire du checkpoint\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Créer le répertoire s'il n'existe pas\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "# === TRAIN PART 1 ===\n",
        "#features = articles_to_text(train_data, \"article_text\", 0, 80000)\n",
        "\n",
        "# === TRAIN PART 2 ===\n",
        "features = articles_to_text(train_data, \"article_text\", 0, len(train_data)-1)\n",
        "\n",
        "# Créer un callback pour sauvegarder les poids du modèle\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Training GPT-2 model...\")\n",
        "\n",
        "# Entraîner le modèle avec GPU et utiliser le callback de checkpoint\n",
        "with tf.device('/device:GPU:0'):\n",
        "    num_epochs = 1\n",
        "\n",
        "    # Taux d'apprentissage décroissant linéairement\n",
        "    learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
        "        initial_learning_rate=5e-5,\n",
        "        decay_steps=len(features) * num_epochs,\n",
        "        end_learning_rate=0.01,\n",
        "    )\n",
        "\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    gpt2_lm.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=loss,\n",
        "        weighted_metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    # Entraîner le modèle avec le callback de checkpoint\n",
        "    gpt2_lm.fit(\n",
        "        x=features,\n",
        "        epochs=num_epochs,\n",
        "        callbacks=[cp_callback]  # Passer le callback de checkpoint à l'entraînement\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP4eBx8m3jEW",
        "outputId": "35afb63c-367c-42a4-9400-4b2e254fe3c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model from loaded model checkpoints...\n",
            "1248/1248 [==============================] - ETA: 0s - loss: 2.9164 - accuracy: 0.4298\n",
            "Epoch 1: saving model to /content/drive/MyDrive/training_data_all_3/cp.ckpt\n",
            "1248/1248 [==============================] - 1624s 1s/step - loss: 2.9164 - accuracy: 0.4298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Test évaluation du modèle\n",
        "\n",
        "Après avoir effectué le fine-tuning du modèle GPT-2, nous avons mené des tests d'évaluation simples.\n",
        "\n",
        "Des tests plus approfondis seront menés dans les prochaines phases de projet."
      ],
      "metadata": {
        "id": "KzuHTE6BjQ9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger les données de validation\n",
        "val_data, val_labels = load_ds.load_dataset(pathToDataset, \"val\", 7)\n",
        "\n",
        "# Rassembler le texte des articles en une seule liste\n",
        "val_features = articles_to_text(val_data, \"article_text\", 0, len(val_data)-1) # 0-6632"
      ],
      "metadata": {
        "id": "qToW0s-nFkIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluer le modèle sur les données d'évaluation\n",
        "with tf.device('/device:GPU:0'):\n",
        "  val_loss, val_accuracy = gpt2_lm.evaluate(\n",
        "      x=val_features,\n",
        "      verbose=1\n",
        "  )\n",
        "\n",
        "# Afficher les résultats de l'évaluation\n",
        "print(f\"Validation Loss: {val_loss}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")"
      ],
      "metadata": {
        "id": "2Wash0RPe8tT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5260e77a-687b-4b55-a94e-b4fa6eeb2332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "208/208 [==============================] - 204s 912ms/step - loss: 2.9360 - sparse_categorical_accuracy: 0.4361\n",
            "Validation Loss: 2.9359724521636963\n",
            "Validation Accuracy: 0.4360581338405609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger les données de test\n",
        "test_data, test_labels = load_ds.load_dataset(pathToDataset, \"test\", 7)\n",
        "\n",
        "# Rassembler le texte des articles en une seule liste\n",
        "test_features = articles_to_text(test_data, \"article_text\", 0, len(test_data)-1) # 0-6657"
      ],
      "metadata": {
        "id": "tEwWJkChFnL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluer le modèle sur les données de test\n",
        "with tf.device('/device:GPU:0'):\n",
        "  test_loss, test_accuracy = gpt2_lm.evaluate(\n",
        "      x=test_features,\n",
        "      verbose=1\n",
        "  )\n",
        "\n",
        "# Afficher les résultats de l'évaluation\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "B4nP7FxBv4K3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12c71cd5-28d1-4d10-a4a5-1634322e7f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "209/209 [==============================] - 194s 905ms/step - loss: 2.9129 - sparse_categorical_accuracy: 0.4390\n",
            "Test Loss: 2.9129157066345215\n",
            "Test Accuracy: 0.43897753953933716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Test génération de texte à partir de prompt\n",
        "\n",
        "Nous avons essayé de générer du texte à partir d'exemple de prompts, afin de vérifier que le modèle reste cohérent après celui, et également pour avoir un aperçu de la spécialisation de celle-ci sur les thèmatiques des articles scientifiques."
      ],
      "metadata": {
        "id": "S3HeNVKwkqJu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNzRrR3GC1ex",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ef9445-c161-4bd4-d0dd-8aa3212a0e37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: The impact of global warming on marine biodiversity\n",
            "GPT-2 output:\n",
            "The impact of global warming on marine biodiversity is well recognized [ 1 , 2 ] . the impact of climate change is expected to be significant for both human and marine ecosystems [ 3 , 4 ] . in addition , the impact of the global climate on human health has been recognized for decades [ 59 ] . in addition , the effects of climate change are also being recognized [ 10 , 1112 ] . in addition to the impact of climate change , it has been recognized in recent years that human and marine ecosystems have a complex interaction . the human ecosystem has many different types of ecosystem , and these interactions may lead to different changes in the environment [\n",
            "TOTAL TIME ELAPSED: 20.92s\n",
            "________________________________________________\n",
            "\n",
            "Prompt: Technological advancements in renewable energy sources\n",
            "GPT-2 output:\n",
            "Technological advancements in renewable energy sources ( e.g. , wind and solar power , geothermal energy sources , and solar energy sources ) have enabled us to generate electricity and water at low temperatures , with a range of applications , for instance , as energy storage , storage , and storage . however , these applications are limited by limited availability for energy storage , and the availability of renewable energy sources in the form of energy storage technologies is limited . in the past , energy storage technologies have been used to produce water , electricity , and water from renewable sources . however , these methods have not been sufficiently applied to meet the demands of the public ,\n",
            "TOTAL TIME ELAPSED: 0.56s\n",
            "________________________________________________\n",
            "\n",
            "Prompt: Genetic factors influencing Alzheimer's disease\n",
            "GPT-2 output:\n",
            "Genetic factors influencing Alzheimer's disease ( ad ) are complex and include genetic factors , which are responsible for the development of the disease . the prevalence of ad in different countries ranges from 0.1% to 3% , and it is estimated that there are about 3,000 new ad cases each year worldwide . ad is a chronic , progressive , neurodegenerative condition with a prevalence of 1.3 - 2 per 10,000 people in developed countries ( 1 ) . ad is caused by genetic factors , which are related to the development of ad in different populations . genetic factors are associated with the development of ad , and the presence\n",
            "TOTAL TIME ELAPSED: 0.97s\n",
            "________________________________________________\n",
            "\n",
            "Prompt: The role of artificial intelligence in personalized medicine\n",
            "GPT-2 output:\n",
            "The role of artificial intelligence in personalized medicine is well known . the concept of personalized medicine is based on a combination of the ability of an animal to identify a disease or a disease - related condition , and its effect on a patient is determined by how a particular condition is treated , and its impact on the individual is determined by how the animal responds to the disease or disease - related condition . the human brain is an animal model that has been developed in order to study the interaction between human and animal systems . it is a model for human disease and for the treatment of cancer and other diseases . the concept of personalized medicine is based on the ability\n",
            "TOTAL TIME ELAPSED: 0.54s\n",
            "________________________________________________\n",
            "\n",
            "Prompt: Quantum computing and its future implications\n",
            "GPT-2 output:\n",
            "Quantum computing and its future implications are the focus of this issue , especially in the field of nanotechnology and nanotechnology . the use of the quantum information has been the subject of much discussion in recent years . it is generally accepted that quantum information is the most important component of information in the field , and quantum information in the field of nanotechnology is of special interest in the field of nanotechnology . the field of quantum information is not confined to nanotechnology . it can be applied to other applications such as quantum medicine and quantum field - effect transistors ( qps ) . however , qps are currently the subject of considerable debate in\n",
            "TOTAL TIME ELAPSED: 0.56s\n",
            "________________________________________________\n",
            "\n",
            "Prompt: Mechanisms of resistance to antibiotics in bacteria\n",
            "GPT-2 output:\n",
            "Mechanisms of resistance to antibiotics in bacteria include the formation of an antibiotic - resistant microorganism ( bor ) and the production of a proinflammatory cytokine ( e.g. , il-1 ) . the development of bor has been associated with a higher rate of resistance to antimicrobials , such as vancomycin - resistant staphylococcus aureus ( vancomycin ) . vancomycin - resistant strains have been identified in various human infections and are considered as one of the most important causes for antibiotic resistance in the world . the resistance to vancomycin is caused\n",
            "TOTAL TIME ELAPSED: 0.55s\n",
            "________________________________________________\n",
            "\n",
            "Prompt: \n",
            "GPT-2 output:\n",
            "the world health organization ( who ) has classified obesity as the fifth leading cause of mortality , disability , and health care costs . in iran , it is defined as a state of hypertriglyceridemia ( triglycerides > 10 mg  dl ) with a high prevalence of type 2 diabetes ( 2.2% ) and hypertension ( 7.9% ) . this disease is associated with a significant risk for cardiovascular disease and diabetes , as well as with a high prevalence of cardiovascular diseases . it is also associated with a higher rate of hypertension ( 8.4% ) and elevated mortality . the prevalence of diabetes has increased worldwide\n",
            "TOTAL TIME ELAPSED: 0.55s\n",
            "________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Exemple de prompts basés sur des sujets de papiers scientifiques\n",
        "prompts = [\n",
        "    \"The impact of global warming on marine biodiversity\",\n",
        "    \"Technological advancements in renewable energy sources\",\n",
        "    \"Genetic factors influencing Alzheimer's disease\",\n",
        "    \"The role of artificial intelligence in personalized medicine\",\n",
        "    \"Quantum computing and its future implications\",\n",
        "    \"Mechanisms of resistance to antibiotics in bacteria\",\n",
        "    \"\"\n",
        "]\n",
        "\n",
        "# Parcourir et générer des réponses pour chaque prompt\n",
        "for prompt in prompts:\n",
        "    start = time.time()\n",
        "    output = gpt2_lm.generate(prompt, max_length=200)\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(\"GPT-2 output:\")\n",
        "    print(output)\n",
        "    print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")\n",
        "    print('________________________________________________')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}