Lors de notre première session, nous avons procédé à la répartition des tâches et entamé notre phase de documentation afin de cerner les différentes possibilités pour notre projet. J'ai été attribué à la tâche délicate du fine-tuning du modèle. Durant cette période, j'ai consacré mon temps à une recherche approfondie, explorant divers blogs et articles pour mieux comprendre les techniques de fine-tuning. Mon attention s'est particulièrement portée sur l'utilisation des transformers, et j'ai décidé d'opter pour le fine-tuning de GPT-2. À la fin de cette session, j'ai consolidé mes connaissances en suivant plusieurs tutoriels disponibles en ligne, notamment sur la plateforme Hugging Face, afin de me familiariser avec les meilleures pratiques en la matière. Ensuite, Alexandre et moi avons entamé le processus de fine-tuning de GPT-2 en utilisant les transformers. Cependant, nous avons rapidement réalisé que la complexité du sujet rendait la compréhension de son fonctionnement ardue. Malgré nos efforts, nous avons été confrontés à une documentation abondante, parfois difficile à assimiler, et nous avons eu du mal à progresser de manière significative. Les multiples ressources consultées n'ont pas toujours été claires, laissant place à une certaine confusion. Cette période a été marquée par une immersion intense dans la documentation, mais malheureusement, notre compréhension pratique du fine-tuning n'a pas évolué comme espéré.
