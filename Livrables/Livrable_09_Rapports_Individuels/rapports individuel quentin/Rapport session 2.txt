Lors de la seconde session, le professeur nous a conseillé d'abandonner l'utilisation des transformers et de nous concentrer exclusivement sur Keras. Bien que cette décision ait été motivée par la volonté de simplifier le processus, nous avons dû faire face à d'importants problèmes de mémoire vive (RAM) sur google collab. Pour surmonter cet obstacle, nous avons adopté une approche progressive en fractionnant le traitement par batch avec une taille de 18, permettant ainsi de faire fonctionner le modèle. Malgré les contraintes de mémoire, cette méthode nous a permis de constater des résultats encourageants, permettant l'entraînement du modèle bien que l'entraînement sur un article mettait environ 5 minutes. Cette adaptation à l'utilisation exclusive de Keras a ouvert de nouvelles perspectives et a marqué un tournant dans notre approche du fine-tuning de l'IA.